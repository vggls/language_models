{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vggls/language_models/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9d6956a",
      "metadata": {
        "id": "f9d6956a"
      },
      "source": [
        "### General imports for all models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "11b7f5a2",
      "metadata": {
        "id": "11b7f5a2"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for google colab import run this cell as well\n",
        "import nltk\n",
        "nltk.download('treebank')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BB5XpXe6SWiW",
        "outputId": "5e60e2cf-e9d1-410e-c7bf-f171b0dd407e"
      },
      "id": "BB5XpXe6SWiW",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import treebank"
      ],
      "metadata": {
        "id": "tV-3ekEQRRsP"
      },
      "id": "tV-3ekEQRRsP",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#custom written code\n",
        "from preprocessing import lower, add_unk_tokens_for_training, replace_with_unk_for_testing, create_ngrams"
      ],
      "metadata": {
        "id": "0pRz-0_KTEZD"
      },
      "id": "0pRz-0_KTEZD",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "5812bcd0",
      "metadata": {
        "id": "5812bcd0"
      },
      "source": [
        "### 3-gram language model with Laplace smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "84f0bf54",
      "metadata": {
        "id": "84f0bf54"
      },
      "outputs": [],
      "source": [
        "# custom written code\n",
        "from laplace_model import count_n_grams, laplace_model, perplexity_ngram_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "aba7fbc4",
      "metadata": {
        "id": "aba7fbc4",
        "outputId": "1cc54396-ee55-4b4e-e997-0f2259e94927",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3576, 338)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Penn Treebank\n",
        "train_treebank = []\n",
        "for j in range(175): # len(treebank.fileids()) = 199\n",
        "    for i in treebank.sents(treebank.fileids()[j]):\n",
        "        l = [j for j in i if '*' not in j] # remove tokens that contain '*'\n",
        "        train_treebank.append(l)\n",
        "\n",
        "test_treebank = []\n",
        "for j in range(175, 199):\n",
        "    for i in treebank.sents(treebank.fileids()[j]):\n",
        "        l = [j for j in i if '*' not in j]\n",
        "        test_treebank.append(l)\n",
        "\n",
        "len(train_treebank), len(test_treebank)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "86e914c7",
      "metadata": {
        "id": "86e914c7"
      },
      "outputs": [],
      "source": [
        "#lower first letter of each token\n",
        "train_tokenized_sentences = lower(train_treebank)\n",
        "test_tokenized_sentences = lower(test_treebank)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "799c8441",
      "metadata": {
        "id": "799c8441"
      },
      "outputs": [],
      "source": [
        "# replace all tokens that appear less than 3 times with <unk>\n",
        "train_tokenized_sentences = add_unk_tokens_for_training(train_tokenized_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "45ad9dfc",
      "metadata": {
        "id": "45ad9dfc",
        "outputId": "6c918aba-f412-4c14-a4e8-2b66bbecf06c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3481"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "#the vocabulary is useful for the testing phase\n",
        "vocabulary = set([item for sublist in train_tokenized_sentences for item in sublist])\n",
        "len(vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "7c85adba",
      "metadata": {
        "id": "7c85adba",
        "outputId": "b603227a-c21e-4b50-c3fc-68fd4678bc63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True, False, False)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "'<unk>' in vocabulary, '<bos>' in vocabulary, '<eos>' in vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_tokenized_sentences = replace_with_unk_for_testing(vocabulary, test_tokenized_sentences)"
      ],
      "metadata": {
        "id": "C6WXQ6T9ineR"
      },
      "id": "C6WXQ6T9ineR",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compute ngrams\n",
        "train_bigrams = create_ngrams(2, train_tokenized_sentences)\n",
        "train_trigrams = create_ngrams(3, train_tokenized_sentences)\n",
        "test_trigrams = create_ngrams(3, test_tokenized_sentences)\n",
        "\n",
        "len(train_bigrams), len(train_trigrams), len(test_trigrams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGkUYXgmH5mQ",
        "outputId": "32a2dd30-e5b2-45fa-fc37-eae2c0e24bbc"
      },
      "id": "xGkUYXgmH5mQ",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(90748, 94324, 8687)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "45022c9c",
      "metadata": {
        "id": "45022c9c",
        "outputId": "73ac0327-a0dd-4f03-e0fa-bdc06e9ec967",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.'] \n",
            "\n",
            "['<unk>', '<unk>', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov.', '29', '.'] \n",
            "\n",
            "[['<bos>', '<unk>'], ['<unk>', '<unk>'], ['<unk>', ','], [',', '61'], ['61', 'years'], ['years', 'old'], ['old', ','], [',', 'will'], ['will', 'join'], ['join', 'the'], ['the', 'board'], ['board', 'as'], ['as', 'a'], ['a', 'nonexecutive'], ['nonexecutive', 'director'], ['director', 'nov.'], ['nov.', '29'], ['29', '.'], ['.', '<eos>']] \n",
            "\n",
            "[['<bos>', '<bos>', '<unk>'], ['<bos>', '<unk>', '<unk>'], ['<unk>', '<unk>', ','], ['<unk>', ',', '61'], [',', '61', 'years'], ['61', 'years', 'old'], ['years', 'old', ','], ['old', ',', 'will'], [',', 'will', 'join'], ['will', 'join', 'the'], ['join', 'the', 'board'], ['the', 'board', 'as'], ['board', 'as', 'a'], ['as', 'a', 'nonexecutive'], ['a', 'nonexecutive', 'director'], ['nonexecutive', 'director', 'nov.'], ['director', 'nov.', '29'], ['nov.', '29', '.'], ['29', '.', '<eos>'], ['.', '<eos>', '<eos>']]\n"
          ]
        }
      ],
      "source": [
        "#example of 2-grams and 3-grams extracted from the first training sentence\n",
        "print(train_treebank[0], '\\n')\n",
        "print(train_tokenized_sentences[0], '\\n')\n",
        "print(train_bigrams[:19], '\\n')\n",
        "print(train_trigrams[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "91a6d6f0",
      "metadata": {
        "id": "91a6d6f0",
        "outputId": "9944843e-264f-469c-d4ba-d579f25f83b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Xerox', 'Corp.', 'has', 'told', 'employees', 'in', 'its', 'Crum', '&', 'Forster', 'personal', 'insurance', 'operations', 'that', 'it', 'is', 'laying', 'off', 'about', '300', 'people', ',', 'or', '25', '%', 'of', 'the', 'staff', '.'] \n",
            "\n",
            "['<unk>', 'corp.', 'has', 'told', 'employees', 'in', 'its', '<unk>', '&', '<unk>', 'personal', 'insurance', 'operations', 'that', 'it', 'is', '<unk>', 'off', 'about', '300', 'people', ',', 'or', '25', '%', 'of', 'the', 'staff', '.'] \n",
            "\n",
            "[['<bos>', '<bos>', '<unk>'], ['<bos>', '<unk>', 'corp.'], ['<unk>', 'corp.', 'has'], ['corp.', 'has', 'told'], ['has', 'told', 'employees'], ['told', 'employees', 'in'], ['employees', 'in', 'its'], ['in', 'its', '<unk>'], ['its', '<unk>', '&'], ['<unk>', '&', '<unk>'], ['&', '<unk>', 'personal'], ['<unk>', 'personal', 'insurance'], ['personal', 'insurance', 'operations'], ['insurance', 'operations', 'that'], ['operations', 'that', 'it'], ['that', 'it', 'is'], ['it', 'is', '<unk>'], ['is', '<unk>', 'off'], ['<unk>', 'off', 'about'], ['off', 'about', '300'], ['about', '300', 'people'], ['300', 'people', ','], ['people', ',', 'or'], [',', 'or', '25'], ['or', '25', '%'], ['25', '%', 'of'], ['%', 'of', 'the'], ['of', 'the', 'staff'], ['the', 'staff', '.'], ['staff', '.', '<eos>'], ['.', '<eos>', '<eos>']]\n"
          ]
        }
      ],
      "source": [
        "#example of 3-grams extracted from the first test sentence\n",
        "print(test_treebank[0], '\\n')\n",
        "print(test_tokenized_sentences[0], '\\n')\n",
        "print(test_trigrams[:31])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "d778d725",
      "metadata": {
        "id": "d778d725"
      },
      "outputs": [],
      "source": [
        "#2-grams and 3-grams frequencies\n",
        "bigrams_counts = count_n_grams(train_bigrams)\n",
        "trigrams_counts = count_n_grams(train_trigrams)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "perplexity_ngram_model(bigrams_counts,\n",
        "                       trigrams_counts,\n",
        "                       test_trigrams,\n",
        "                       len(vocabulary))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gql3hv_vVIcU",
        "outputId": "a28327bc-2ec5-4fdf-f8ac-f650f172d05d"
      },
      "id": "Gql3hv_vVIcU",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8622973986362099"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d91ea29",
      "metadata": {
        "id": "8d91ea29"
      },
      "source": [
        "### LSTM language model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TO BE TRANSFERED INTO README.md\n",
        "\n",
        "https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\n",
        "\n",
        "*In this tutorial, you will split the book text up into subsequences with a fixed length of 100 characters, an arbitrary length. You could just as easily split the data by sentences, padding the shorter sequences and truncating the longer ones.* (This is why we say that recurrent nets can handle arbitrary size inputs; because we either put all sentences together or handle them one by one even if the have different length.)\n",
        "\n",
        "*Each training pattern of the network comprises 100 time steps of one character (X) followed by one character output (y). When creating these sequences, you slide this window along the whole book one character at a time, allowing each character a chance to be learned from the 100 characters that preceded it (except the first 100 characters, of course).*"
      ],
      "metadata": {
        "id": "gzgAXagzsN1Y"
      },
      "id": "gzgAXagzsN1Y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55ba18ac",
      "metadata": {
        "id": "55ba18ac"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d41a1600",
      "metadata": {
        "id": "d41a1600"
      },
      "outputs": [],
      "source": [
        "#custom written code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca932ef4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca932ef4",
        "outputId": "edc16371-cc4f-4b66-98d2-c5f61afa7b04"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3262, 314, 338)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Penn Treebank\n",
        "train_treebank = []\n",
        "for j in range(150): # len(treebank.fileids()) = 199\n",
        "    for i in treebank.sents(treebank.fileids()[j]):\n",
        "        l = [j for j in i if '*' not in j] # remove tokens that contain '*'\n",
        "        train_treebank.append(l)\n",
        "\n",
        "val_treebank = []\n",
        "for j in range(150, 175): # len(treebank.fileids()) = 199\n",
        "    for i in treebank.sents(treebank.fileids()[j]):\n",
        "        l = [j for j in i if '*' not in j] # remove tokens that contain '*'\n",
        "        val_treebank.append(l)\n",
        "\n",
        "test_treebank = []\n",
        "for j in range(175, 199):\n",
        "    for i in treebank.sents(treebank.fileids()[j]):\n",
        "        l = [j for j in i if '*' not in j]\n",
        "        test_treebank.append(l)\n",
        "\n",
        "len(train_treebank), len(val_treebank), len(test_treebank)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80c3f077",
      "metadata": {
        "id": "80c3f077"
      },
      "outputs": [],
      "source": [
        "#lower first letter of each token\n",
        "train_tokenized_sentences = lower(train_treebank)\n",
        "val_tokenized_sentences = lower(val_treebank)\n",
        "test_tokenized_sentences = lower(test_treebank)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "924e91fb",
      "metadata": {
        "id": "924e91fb"
      },
      "outputs": [],
      "source": [
        "# replace all tokens that appear less than 3 times with <UNK>\n",
        "train_tokenized_sentences = add_unk_tokens(train_tokenized_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76060004",
      "metadata": {
        "id": "76060004"
      },
      "source": [
        "The vocabulary is constructed by the training data only. Note that the training data is different between the 3-gram and the lstm model, because the later one needs validation as well (in order to hyper-tune; note that the 3-gram model is unique). Since the test set will be the same for all models, for the lstm model we use as training set the largest part of the 3-gram model training set and the remaining small part as validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "820386ec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "820386ec",
        "outputId": "b758cd59-14ff-4238-ecf7-5c32972f2143"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3273"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "#the vocabulary is useful for the testing phase\n",
        "vocabulary = set([item for sublist in train_tokenized_sentences for item in sublist])\n",
        "len(vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f15ee84",
      "metadata": {
        "id": "6f15ee84"
      },
      "outputs": [],
      "source": [
        "#Insert <EOS> token in the vocabulary? see source argument at the respective \"Vocabulary\" section\n",
        "# BUT in my case I have included \".\", whereas they do not. So, can i say that \".\" plays the role of \"<EOS>\"? Why not?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8964f5a1",
      "metadata": {
        "id": "8964f5a1"
      },
      "outputs": [],
      "source": [
        "# not the same index assignments every time i run the cell\n",
        "word_to_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
        "index_to_word = {idx: word for word, idx in word_to_index.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20f69823",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20f69823",
        "outputId": "8d490606-2c36-4a4f-ae89-fd7cffd078fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(857, 1965)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "word_to_index['.'], word_to_index['<unk>']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d20c19a3",
      "metadata": {
        "id": "d20c19a3"
      },
      "outputs": [],
      "source": [
        "# duplicate code - write function\n",
        "\n",
        "# training\n",
        "train_sequences = [[word_to_index[word] for word in sentence] for sentence in train_tokenized_sentences]\n",
        "train_sequence = []\n",
        "for seq in train_sequences:\n",
        "    train_sequence.extend(seq)\n",
        "\n",
        "# validation\n",
        "for sent in val_tokenized_sentences:\n",
        "    for i, token in enumerate(sent):\n",
        "        if token not in vocabulary:\n",
        "            sent[i] = '<unk>'\n",
        "val_sequences = [[word_to_index[word] for word in sentence] for sentence in val_tokenized_sentences]\n",
        "\n",
        "val_sequence = []\n",
        "for seq in val_sequences:\n",
        "    val_sequence.extend(seq)\n",
        "\n",
        "# testing\n",
        "for sent in test_tokenized_sentences:\n",
        "    for i, token in enumerate(sent):\n",
        "        if token not in vocabulary:\n",
        "            sent[i] = '<unk>'\n",
        "test_sequences = [[word_to_index[word] for word in sentence] for sentence in test_tokenized_sentences]\n",
        "\n",
        "test_sequence = []\n",
        "for seq in test_sequences:\n",
        "    test_sequence.extend(seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b397dcbb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b397dcbb",
        "outputId": "4c3dead3-c896-49f7-923b-ebbbcdc35230"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(79427, 7745, 8011)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "len(train_sequence), len(val_sequence), len(test_sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efdf6726",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efdf6726",
        "outputId": "4d8205c3-d354-440e-aa6b-54846daf7d77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<unk>', '<unk>', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov.', '29', '.'] ['mr.', '<unk>', 'is', 'chairman', 'of', '<unk>', 'n.v.', ',', 'the', 'dutch', 'publishing', 'group', '.'] \n",
            "\n",
            "[1965, 1965, 1388, 1999, 1051, 758, 1388, 2209, 2410, 955, 3191, 2587, 2882, 3178, 2374, 2203, 63, 857] [816, 1965, 820, 3238, 2864, 1965, 1707, 1388, 955, 3170, 20, 1034, 857] \n",
            "\n",
            "[1965, 1965, 1388, 1999, 1051, 758, 1388, 2209, 2410, 955, 3191, 2587, 2882, 3178, 2374, 2203, 63, 857, 816, 1965, 820, 3238, 2864, 1965, 1707, 1388, 955, 3170, 20, 1034, 857]\n"
          ]
        }
      ],
      "source": [
        "# brief explanation how to feed a recurrent neural net\n",
        "# for simplicity, consider the case of the first two sentences\n",
        "print(train_tokenized_sentences[0], train_tokenized_sentences[1], '\\n')\n",
        "print(train_sequences[0], train_sequences[1], '\\n')\n",
        "print(train_sequence[:31])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c8939d2",
      "metadata": {
        "id": "1c8939d2"
      },
      "source": [
        "In the above representation recall that '.' is represented by 2262 and the unknown word by 1855.\n",
        "\n",
        "So if we process the data in sequences of length = 5, the model will learn as follows:\n",
        "\n",
        "- map [1855, 1855, 1062, 419] to 1620\n",
        "- map [1855, 1062, 419, 1620] to 885\n",
        "- i.e. shift input by 1-step to the future and continue like this"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title LSTM model\n",
        "\n",
        "# https://stackoverflow.com/questions/49224413/difference-between-1-lstm-with-num-layers-2-and-2-lstms-in-pytorch\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, tie_weights):\n",
        "        super(LSTMModel, self).__init__()\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim,\n",
        "                            hidden_size=hidden_dim,\n",
        "                            num_layers = num_layers,\n",
        "                            batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_rate)          #regulazirer\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "        if tie_weights:\n",
        "            assert embedding_dim == hidden_dim\n",
        "            self.embedding.weight = self.fc.weight\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, x, h, c):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, (hidden, cell) = self.lstm(embedded, (h, c))\n",
        "        dropout_out = self.dropout(lstm_out)\n",
        "        output = self.fc(dropout_out[:, -1, :])\n",
        "        return output, (hidden, cell)\n",
        "\n",
        "    def init_weights(self):\n",
        "        init_range_emb = 0.1\n",
        "        init_range_other = 1/math.sqrt(self.hidden_dim)\n",
        "        self.embedding.weight.data.uniform_(-init_range_emb, init_range_emb)\n",
        "        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n",
        "        self.fc.bias.data.zero_()\n",
        "        for i in range(self.num_layers):\n",
        "            self.lstm.all_weights[i][0] = torch.FloatTensor(self.embedding_dim,\n",
        "                    self.hidden_dim).uniform_(-init_range_other, init_range_other)\n",
        "            self.lstm.all_weights[i][1] = torch.FloatTensor(self.hidden_dim,\n",
        "                    self.hidden_dim).uniform_(-init_range_other, init_range_other)"
      ],
      "metadata": {
        "id": "jzsBJUb7rTL4"
      },
      "id": "jzsBJUb7rTL4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "640ed70a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "640ed70a",
        "outputId": "dad1f733-e588-44a7-8b41-f7ecb8f56a73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of trainable parameters: 1893833\n"
          ]
        }
      ],
      "source": [
        "#model architecture hyperparams\n",
        "vocab_size = len(vocabulary)\n",
        "embedding_dim = 256\n",
        "num_layers = 2\n",
        "hidden_dim = 256\n",
        "output_dim = vocab_size\n",
        "dropout_rate = 0.3\n",
        "\n",
        "#model training hyperparams\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Create the model, loss function, and optimizer\n",
        "model = LSTMModel(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, True)\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'No. of trainable parameters: {num_params}')\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title this is an important remark\n",
        "\n",
        "#num_train_batches = len(train_sequence)//batch_size #+ 1*(len(train_sequence)%batch_size != 0)\n",
        "#num_val_batches = len(val_sequence)//batch_size #+ 1*(len(val_sequence)%batch_size != 0)\n",
        "\n",
        "# the last batch will consist of less than 128 30-word sequences\n",
        "# and it is ok to compute them as well.. but there is a problem with the shape of the (hidden, cell) dimensions..\n",
        "# as hidden.shape[1] and cell.shape[1] is no longer batch_size (i.e. 128)\n",
        "# i have not found any way to overcome this, so for now I will not use these last points\n",
        "# Note that I cannot reset the (hidden,cell) values to zero for the last smaller batch,\n",
        "#because i will lose the computed ones, which is wrong\n",
        "\n",
        "#num_train_batches, num_val_batches"
      ],
      "metadata": {
        "id": "l-7NQB7k5MdX",
        "cellView": "form"
      },
      "id": "l-7NQB7k5MdX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from training import Train"
      ],
      "metadata": {
        "id": "-VnD90xSOsTx"
      },
      "id": "-VnD90xSOsTx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instance = Train(model=model,\n",
        "                loss_fct=criterion,\n",
        "                optimizer=optimizer,\n",
        "                train_sequence=train_sequence,\n",
        "                val_sequence=val_sequence,\n",
        "                sequence_length=50,\n",
        "                batch_size=128,\n",
        "                epochs=50,\n",
        "                patience=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5WVNxn_Wjog",
        "outputId": "49335fd7-ffc6-41a2-87e9-b18c7e9a8533"
      },
      "id": "j5WVNxn_Wjog",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding_dim = 256, num_layers = 2, hidden_dim = 256, dropout_rate = 0.3, learning_rate = 0.001\n",
        "# sequence_length=50, batch_size=128, epochs=50, patience=5\n",
        "train_loss, val_loss, checkpoints = instance.training()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaXRSkLpgvVv",
        "outputId": "a42725ef-febc-4c41-efcc-131e419ba4ed"
      },
      "id": "YaXRSkLpgvVv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training..\n",
            "Epoch: 1/50 - Training: loss 6.075- Validation: loss 5.304, perplexity 201.209\n",
            "Epoch: 2/50 - Training: loss 5.601 - Validation: loss 5.059, perplexity 157.506 - E.S. checkpoint\n",
            "Epoch: 3/50 - Training: loss 5.384 - Validation: loss 4.910, perplexity 135.678 - E.S. checkpoint\n",
            "Epoch: 4/50 - Training: loss 5.197 - Validation: loss 4.814, perplexity 123.175 - E.S. checkpoint\n",
            "Epoch: 5/50 - Training: loss 5.034 - Validation: loss 4.730, perplexity 113.351 - E.S. checkpoint\n",
            "Epoch: 6/50 - Training: loss 4.894 - Validation: loss 4.668, perplexity 106.465 - E.S. checkpoint\n",
            "Epoch: 7/50 - Training: loss 4.765 - Validation: loss 4.635, perplexity 103.077 - E.S. checkpoint\n",
            "Epoch: 8/50 - Training: loss 4.644 - Validation: loss 4.595, perplexity 98.949 - E.S. checkpoint\n",
            "Epoch: 9/50 - Training: loss 4.536 - Validation: loss 4.576, perplexity 97.147 - E.S. checkpoint\n",
            "Epoch: 10/50 - Training: loss 4.424 - Validation: loss 4.548, perplexity 94.402 - E.S. checkpoint\n",
            "Epoch: 11/50 - Training: loss 4.320 - Validation: loss 4.537, perplexity 93.456 - E.S. checkpoint\n",
            "Epoch: 12/50 - Training: loss 4.222 - Validation: loss 4.526, perplexity 92.431 - E.S. checkpoint\n",
            "Epoch: 13/50 - Training: loss 4.146 - Validation: loss 4.522, perplexity 92.035 - E.S. checkpoint\n",
            "Epoch: 14/50 - Training: loss 4.047- Validation: loss 4.524, perplexity 92.173\n",
            "Epoch: 15/50 - Training: loss 3.962- Validation: loss 4.524, perplexity 92.199\n",
            "Epoch: 16/50 - Training: loss 3.884- Validation: loss 4.534, perplexity 93.124\n",
            "Epoch: 17/50 - Training: loss 3.796- Validation: loss 4.523, perplexity 92.122\n",
            "Epoch: 18/50 - Training: loss 3.705- Validation: loss 4.551, perplexity 94.750\n",
            "Training complete !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# embedding_dim = 512, num_layers = 2, hidden_dim = 512, dropout_rate = 0.4, learning_rate = 0.001\n",
        "# sequence_length=30, batch_size=256, epochs=50, patience=5\n",
        "# good configuration as well"
      ],
      "metadata": {
        "id": "SHRgQnWBXjx8"
      },
      "id": "SHRgQnWBXjx8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed0c9555",
      "metadata": {
        "id": "ed0c9555",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title notebook training loop\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "\n",
        "    (hidden, cell) = (torch.zeros(num_layers, batch_size, hidden_dim).to(device),\n",
        "                      torch.zeros(num_layers, batch_size, hidden_dim).to(device))\n",
        "\n",
        "    for n in range(num_train_batches):\n",
        "\n",
        "        batch = train_sequences[n*batch_size:(n+1)*batch_size]\n",
        "        batch = torch.tensor(batch)\n",
        "        # batch.shape is (batch_size, sequence_length+1)\n",
        "\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()                             #set gradients to zero before back prop\n",
        "\n",
        "        hidden.detach_()\n",
        "        cell.detach_()\n",
        "\n",
        "        output, (hidden, cell) = model(batch[:,:-1], hidden, cell)      #forward\n",
        "\n",
        "        target = batch[:,-1]                              #set target\n",
        "\n",
        "        loss = criterion(output, target)                  #compute loss\n",
        "\n",
        "        loss.backward()                                   #backprop\n",
        "        optimizer.step()                                  #update weights\n",
        "\n",
        "        train_loss += loss.item()                         #accumulate batch loss\n",
        "\n",
        "    train_loss = train_loss / num_train_batches           #avg epoch loss\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "\n",
        "    with torch.no_grad(): #since in validation phase there is no backprop and weight updates\n",
        "\n",
        "        (hidden, cell) = (torch.zeros(num_layers, batch_size, hidden_dim).to(device),\n",
        "                          torch.zeros(num_layers, batch_size, hidden_dim).to(device))\n",
        "\n",
        "        for n in range(num_val_batches):\n",
        "\n",
        "            batch = val_sequences[n*batch_size:(n+1)*batch_size]\n",
        "            batch = torch.tensor(batch)\n",
        "\n",
        "            batch = batch.to(device)\n",
        "\n",
        "            hidden.detach_()\n",
        "            cell.detach_()\n",
        "\n",
        "            output, (hidden, cell) = model(batch[:,:-1], hidden, cell)\n",
        "            target = batch[:,-1]\n",
        "            loss = criterion(output, target)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "        val_loss /= num_val_batches\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.3f}, Val Loss: {val_loss:.3f}, Val Perplexity: {math.exp(val_loss):.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to replace the one in github\n",
        "import math\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "def perplexity_neural_model(model, test_sequence, sequence_length, criterion):\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    loss = 0\n",
        "\n",
        "    num_layers = model.lstm.num_layers\n",
        "    hidden_dim = model.lstm.hidden_size\n",
        "    test_sequences = [test_sequence[i:i+sequence_length+1] for i in range(0, len(test_sequence)-sequence_length)]\n",
        "\n",
        "    with torch.no_grad(): #since in validation phase there is no backprop and weight updates\n",
        "\n",
        "        (hidden, cell) = (torch.zeros(num_layers, 1, hidden_dim).to(device),\n",
        "                          torch.zeros(num_layers, 1, hidden_dim).to(device))\n",
        "\n",
        "        for seq in test_sequences:\n",
        "\n",
        "            seq = torch.tensor(seq)\n",
        "            seq = seq.view(1, -1)\n",
        "            seq = seq.to(device)\n",
        "            output, (hidden, cell) = model(seq[:,:-1], hidden, cell)\n",
        "            target = seq[:,-1]\n",
        "            #print(output.shape, target.shape)\n",
        "            loss = criterion(output, target)\n",
        "            loss += loss.item()\n",
        "\n",
        "        loss /= len(test_sequences)\n",
        "\n",
        "    perplexity = math.exp(loss)\n",
        "\n",
        "    return perplexity"
      ],
      "metadata": {
        "id": "wsJPXl9BR8ms"
      },
      "id": "wsJPXl9BR8ms",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cdc34fa",
      "metadata": {
        "id": "9cdc34fa"
      },
      "outputs": [],
      "source": [
        "perplexity_neural_model(model, test_sequence, 30, criterion)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da2fd2ac",
      "metadata": {
        "id": "da2fd2ac"
      },
      "source": [
        "### Pre-Trained Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb9d9c82",
      "metadata": {
        "id": "eb9d9c82"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4db639c2",
      "metadata": {
        "id": "4db639c2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81a6c501",
      "metadata": {
        "id": "81a6c501"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "72762156",
      "metadata": {
        "id": "72762156"
      },
      "source": [
        "### Comparisons & Text generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3c9b5fe",
      "metadata": {
        "id": "b3c9b5fe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f8ebf1f",
      "metadata": {
        "id": "4f8ebf1f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "f9d6956a",
        "5812bcd0",
        "da2fd2ac",
        "72762156"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}