This is an introductory repo to different architectures of language models, trained and tested on the Penn Treebank. We discuss:
- A. 3-gram language model with Laplace smoothing
- B. LSTM neural language model (build from scratch)
- C. LSTM neural language model with pretrained embeddings
- D. Transformer model
