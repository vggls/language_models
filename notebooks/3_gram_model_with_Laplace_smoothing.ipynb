{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vggls/language_models/blob/main/notebooks/3_gram_model_with_Laplace_smoothing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Imports"
      ],
      "metadata": {
        "id": "ZYaBTYnhu-BD"
      },
      "id": "ZYaBTYnhu-BD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11b7f5a2",
      "metadata": {
        "id": "11b7f5a2"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import math\n",
        "#import string #string.punctuation contains punctuation symbols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BB5XpXe6SWiW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BB5XpXe6SWiW",
        "outputId": "c9b0e742-59e6-447c-8b4a-237f44383846"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# for google colab import run this cell as well\n",
        "import nltk\n",
        "nltk.download('treebank')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tV-3ekEQRRsP",
      "metadata": {
        "id": "tV-3ekEQRRsP"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import treebank"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset and Preprocessing"
      ],
      "metadata": {
        "id": "MoDBZ1p7vCB4"
      },
      "id": "MoDBZ1p7vCB4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84f0bf54",
      "metadata": {
        "id": "84f0bf54"
      },
      "outputs": [],
      "source": [
        "# custom written code\n",
        "from preprocessing import lower, add_unk_tokens_for_training, replace_with_unk_for_testing, create_ngrams\n",
        "from laplace_model import count_n_grams, laplace_model, perplexity_ngram_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aba7fbc4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aba7fbc4",
        "outputId": "4255c4c4-43ed-4732-ff41-2d560fcf7b22"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3576, 338)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Penn Treebank\n",
        "symbols_to_remove = set(['-LRB-', '-RRB-', '-LSB-', '-RSB-', '-LCB-', '-RCB-']) # parentheses\n",
        "\n",
        "train_treebank = []\n",
        "for j in range(175):\n",
        "    for i in treebank.sents(treebank.fileids()[j]):\n",
        "        l = [token for token in i if ('*' not in token) and ('\\/' not in token) and (token not in symbols_to_remove)] # Remove tokens that contain '*', '\\/' or symbols_to_remove\n",
        "        train_treebank.append(l) # Append the sentence to the training data\n",
        "\n",
        "test_treebank = []\n",
        "for j in range(175, 199):\n",
        "    for i in treebank.sents(treebank.fileids()[j]):\n",
        "        l = [token for token in i if '*' not in token and token not in symbols_to_remove]\n",
        "        test_treebank.append(l)\n",
        "\n",
        "len(train_treebank), len(test_treebank)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86e914c7",
      "metadata": {
        "id": "86e914c7"
      },
      "outputs": [],
      "source": [
        "#lower first letter of each token\n",
        "train_tokenized_sentences = lower(train_treebank)\n",
        "test_tokenized_sentences = lower(test_treebank)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "799c8441",
      "metadata": {
        "id": "799c8441"
      },
      "outputs": [],
      "source": [
        "# insert <unk> token to training data\n",
        "train_tokenized_sentences = add_unk_tokens_for_training(train_tokenized_sentences) #replace all tokens that appear less than 3 times with <unk>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45ad9dfc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45ad9dfc",
        "outputId": "bba0dae6-9774-483a-8ebf-59868db9d201"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3466"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "vocabulary = set([item for sublist in train_tokenized_sentences for item in sublist])\n",
        "len(vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c85adba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c85adba",
        "outputId": "919c3102-b69c-4c14-d430-41a058799419"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True, False, False)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "'<unk>' in vocabulary, '<bos>' in vocabulary, '<eos>' in vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "C6WXQ6T9ineR",
      "metadata": {
        "id": "C6WXQ6T9ineR"
      },
      "outputs": [],
      "source": [
        "# insert <unk> token to test data\n",
        "test_tokenized_sentences = replace_with_unk_for_testing(vocabulary, test_tokenized_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xGkUYXgmH5mQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGkUYXgmH5mQ",
        "outputId": "0278d00f-68d9-445c-bcaa-21ae1839dae5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(90375, 93951, 8663)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "#\"create_ngrams\" method adds <bos> and <eos> tokens and computes ngrams\n",
        "train_bigrams = create_ngrams(2, train_tokenized_sentences)\n",
        "train_trigrams = create_ngrams(3, train_tokenized_sentences)\n",
        "test_trigrams = create_ngrams(3, test_tokenized_sentences)\n",
        "\n",
        "len(train_bigrams), len(train_trigrams), len(test_trigrams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45022c9c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45022c9c",
        "outputId": "20efdca2-8c78-4e99-a094-c3155d7fcd39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.'] \n",
            "\n",
            "['<unk>', '<unk>', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov.', '29', '.'] \n",
            "\n",
            "[['<bos>', '<unk>'], ['<unk>', '<unk>'], ['<unk>', ','], [',', '61'], ['61', 'years'], ['years', 'old'], ['old', ','], [',', 'will'], ['will', 'join'], ['join', 'the'], ['the', 'board'], ['board', 'as'], ['as', 'a'], ['a', 'nonexecutive'], ['nonexecutive', 'director'], ['director', 'nov.'], ['nov.', '29'], ['29', '.'], ['.', '<eos>']] \n",
            "\n",
            "[['<bos>', '<bos>', '<unk>'], ['<bos>', '<unk>', '<unk>'], ['<unk>', '<unk>', ','], ['<unk>', ',', '61'], [',', '61', 'years'], ['61', 'years', 'old'], ['years', 'old', ','], ['old', ',', 'will'], [',', 'will', 'join'], ['will', 'join', 'the'], ['join', 'the', 'board'], ['the', 'board', 'as'], ['board', 'as', 'a'], ['as', 'a', 'nonexecutive'], ['a', 'nonexecutive', 'director'], ['nonexecutive', 'director', 'nov.'], ['director', 'nov.', '29'], ['nov.', '29', '.'], ['29', '.', '<eos>'], ['.', '<eos>', '<eos>']]\n"
          ]
        }
      ],
      "source": [
        "#example of 2-grams and 3-grams extracted from the first training sentence\n",
        "print(train_treebank[0], '\\n')\n",
        "print(train_tokenized_sentences[0], '\\n')\n",
        "print(train_bigrams[:19], '\\n')\n",
        "print(train_trigrams[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91a6d6f0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91a6d6f0",
        "outputId": "0ed73fb8-cd8d-43bd-dd4e-82e7b178ab65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Xerox', 'Corp.', 'has', 'told', 'employees', 'in', 'its', 'Crum', '&', 'Forster', 'personal', 'insurance', 'operations', 'that', 'it', 'is', 'laying', 'off', 'about', '300', 'people', ',', 'or', '25', '%', 'of', 'the', 'staff', '.'] \n",
            "\n",
            "['<unk>', 'corp.', 'has', 'told', 'employees', 'in', 'its', '<unk>', '&', '<unk>', 'personal', 'insurance', 'operations', 'that', 'it', 'is', '<unk>', 'off', 'about', '300', 'people', ',', 'or', '25', '%', 'of', 'the', 'staff', '.'] \n",
            "\n",
            "[['<bos>', '<bos>', '<unk>'], ['<bos>', '<unk>', 'corp.'], ['<unk>', 'corp.', 'has'], ['corp.', 'has', 'told'], ['has', 'told', 'employees'], ['told', 'employees', 'in'], ['employees', 'in', 'its'], ['in', 'its', '<unk>'], ['its', '<unk>', '&'], ['<unk>', '&', '<unk>'], ['&', '<unk>', 'personal'], ['<unk>', 'personal', 'insurance'], ['personal', 'insurance', 'operations'], ['insurance', 'operations', 'that'], ['operations', 'that', 'it'], ['that', 'it', 'is'], ['it', 'is', '<unk>'], ['is', '<unk>', 'off'], ['<unk>', 'off', 'about'], ['off', 'about', '300'], ['about', '300', 'people'], ['300', 'people', ','], ['people', ',', 'or'], [',', 'or', '25'], ['or', '25', '%'], ['25', '%', 'of'], ['%', 'of', 'the'], ['of', 'the', 'staff'], ['the', 'staff', '.'], ['staff', '.', '<eos>'], ['.', '<eos>', '<eos>']]\n"
          ]
        }
      ],
      "source": [
        "#example of 3-grams extracted from the first test sentence\n",
        "print(test_treebank[0], '\\n')\n",
        "print(test_tokenized_sentences[0], '\\n')\n",
        "print(test_trigrams[:31])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model"
      ],
      "metadata": {
        "id": "b9K8xXcivJqv"
      },
      "id": "b9K8xXcivJqv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d778d725",
      "metadata": {
        "id": "d778d725"
      },
      "outputs": [],
      "source": [
        "#2-grams and 3-grams frequencies\n",
        "bigrams_counts = count_n_grams(train_bigrams)\n",
        "trigrams_counts = count_n_grams(train_trigrams)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('ngrams_counts.pickle', 'wb') as f:\n",
        "        pickle.dump([bigrams_counts, trigrams_counts], f)"
      ],
      "metadata": {
        "id": "OUcSSiQgLq5D"
      },
      "id": "OUcSSiQgLq5D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Perplexity"
      ],
      "metadata": {
        "id": "aNxmZnPTvQM0"
      },
      "id": "aNxmZnPTvQM0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Gql3hv_vVIcU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gql3hv_vVIcU",
        "outputId": "16e5b5ce-e5ca-474c-ede0-789c9a9fa00d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1082.933692249023"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "perplexity_ngram_model(nminus1_grams_counts=bigrams_counts,\n",
        "                       n_grams_counts=trigrams_counts,\n",
        "                       test_n_grams=test_trigrams,\n",
        "                       vocab_size=len(vocabulary))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "ZYaBTYnhu-BD",
        "MoDBZ1p7vCB4",
        "b9K8xXcivJqv",
        "aNxmZnPTvQM0"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}