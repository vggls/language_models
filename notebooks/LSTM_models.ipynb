{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vggls/language_models/blob/main/notebooks/LSTM_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Imports"
      ],
      "metadata": {
        "id": "WZFdTVnWvYHG"
      },
      "id": "WZFdTVnWvYHG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11b7f5a2",
      "metadata": {
        "id": "11b7f5a2"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import math\n",
        "#import string #string.punctuation contains punctuation symbols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BB5XpXe6SWiW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BB5XpXe6SWiW",
        "outputId": "c9b0e742-59e6-447c-8b4a-237f44383846"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# for google colab import run this cell as well\n",
        "import nltk\n",
        "nltk.download('treebank')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tV-3ekEQRRsP",
      "metadata": {
        "id": "tV-3ekEQRRsP"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import treebank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55ba18ac",
      "metadata": {
        "id": "55ba18ac"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d41a1600",
      "metadata": {
        "id": "d41a1600"
      },
      "outputs": [],
      "source": [
        "#custom written code\n",
        "from preprocessing import lower, add_unk_tokens_for_training, unk_for_reduced_vocab, replace_with_unk_for_testing, tokens_to_indices\n",
        "from training import Train\n",
        "from lstm_model import LSTMModel\n",
        "from perplexity_neural import perplexity_neural_model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset and some preprocessing"
      ],
      "metadata": {
        "id": "VG3HM5u6vcBv"
      },
      "id": "VG3HM5u6vcBv"
    },
    {
      "cell_type": "code",
      "source": [
        "# Penn Treebank\n",
        "\n",
        "def load_treebank(left_limit, right_limit):\n",
        "\n",
        "    symbols_to_remove = set(['-LRB-', '-RRB-', '-LSB-', '-RSB-', '-LCB-', '-RCB-']) # parentheses\n",
        "\n",
        "    #sos_token = ['<bos>']\n",
        "    eos_token = ['<eos>']\n",
        "\n",
        "    tokenized_sentences = []\n",
        "    for j in range(left_limit, right_limit):\n",
        "        for i in treebank.sents(treebank.fileids()[j]):\n",
        "            l = [token for token in i if ('*' not in token) and ('\\/' not in token) and (token not in symbols_to_remove)]\n",
        "            l = l + eos_token\n",
        "            tokenized_sentences.append(l)\n",
        "\n",
        "    return tokenized_sentences"
      ],
      "metadata": {
        "id": "gtuhGfTy-a6J"
      },
      "id": "gtuhGfTy-a6J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca932ef4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca932ef4",
        "outputId": "ae68133f-d389-4b24-dacb-89fb0f5bdb95"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3262, 314, 338)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "train_treebank = load_treebank(0, 150)\n",
        "val_treebank = load_treebank(150, 175)\n",
        "test_treebank = load_treebank(175, 199)\n",
        "\n",
        "len(train_treebank), len(val_treebank), len(test_treebank)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80c3f077",
      "metadata": {
        "id": "80c3f077"
      },
      "outputs": [],
      "source": [
        "#lower first letter of each token\n",
        "lower_train_treebank = lower(train_treebank)\n",
        "lower_val_treebank = lower(val_treebank)\n",
        "lower_test_treebank = lower(test_treebank)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "924e91fb",
      "metadata": {
        "id": "924e91fb"
      },
      "outputs": [],
      "source": [
        "# insert <unk> token to training data for case I model\n",
        "train_sentences_I = add_unk_tokens_for_training(lower_train_treebank) #replace all tokens that appear less than 3 times with <unk>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "820386ec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "820386ec",
        "outputId": "709011a4-f849-4621-8e83-3cdbe890ef3b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3259, True, True)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# case I vocabulary\n",
        "vocabulary_I = set([item for sublist in train_sentences_I for item in sublist])\n",
        "len(vocabulary_I), '<unk>' in vocabulary_I, '<eos>' in vocabulary_I"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model architecture hyperparams - for both cases\n",
        "embedding_dim = 300\n",
        "num_layers = 2\n",
        "hidden_dim = 256\n",
        "dropout_rate = 0.3"
      ],
      "metadata": {
        "id": "zZwK1O9nv7xQ"
      },
      "id": "zZwK1O9nv7xQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### case I - model with learnable embeddings: further preprocessing, model training and perplexity (all variable names end in 'I')"
      ],
      "metadata": {
        "id": "Z12KumNdn827"
      },
      "id": "Z12KumNdn827"
    },
    {
      "cell_type": "code",
      "source": [
        "# not the same index assignments every time i run the cell\n",
        "word_to_index_I = {word: idx for idx, word in enumerate(vocabulary_I)}\n",
        "index_to_word_I = {idx: word for word, idx in word_to_index_I.items()}\n",
        "\n",
        "with open('lstm_caseI_word_index_mappings.pickle', 'wb') as f:\n",
        "        pickle.dump([word_to_index_I, index_to_word_I], f)"
      ],
      "metadata": {
        "id": "G_WOQK2Ble5H"
      },
      "id": "G_WOQK2Ble5H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d20c19a3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d20c19a3",
        "outputId": "0458091d-f91e-4486-ddd6-0a5b29243246"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(82372, 8003, 8319)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# training sequence of indices\n",
        "train_int_sequence_I = tokens_to_indices(word_to_index_I, train_sentences_I)\n",
        "\n",
        "# validation sequence of indices\n",
        "val_sentences_I = replace_with_unk_for_testing(vocabulary_I, lower_val_treebank)\n",
        "val_int_sequence_I = tokens_to_indices(word_to_index_I, val_sentences_I)\n",
        "\n",
        "# testing sequence of indices\n",
        "test_sentences_I = replace_with_unk_for_testing(vocabulary_I, lower_test_treebank)\n",
        "test_int_sequence_I = tokens_to_indices(word_to_index_I, test_sentences_I)\n",
        "\n",
        "len(train_int_sequence_I), len(val_int_sequence_I), len(test_int_sequence_I)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(set(train_int_sequence_I)), len(set(val_int_sequence_I)), len(set(test_int_sequence_I))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRDdI05dub1i",
        "outputId": "a6077e85-d2c2-4694-9cdb-ce6959159535"
      },
      "id": "NRDdI05dub1i",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3259, 1165, 1272)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "640ed70a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "640ed70a",
        "outputId": "88e9b954-15e4-43ef-8904-c420e2a2c240"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of trainable parameters: 2912991\n"
          ]
        }
      ],
      "source": [
        "# Create the model, loss function, and optimizer\n",
        "vocabI_size = len(vocabulary_I)\n",
        "model = LSTMModel(vocabI_size, embedding_dim, hidden_dim, num_layers, dropout_rate, None)\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'No. of trainable parameters: {num_params}')\n",
        "\n",
        "#model training hyperparams\n",
        "learning_rate = 0.001\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "#scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "j5WVNxn_Wjog",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5WVNxn_Wjog",
        "outputId": "5e009f4b-cb90-42c5-b155-5c39c08d5816"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "instance = Train(model=model,\n",
        "                model_type = 'lstm',\n",
        "                loss_fct=criterion,\n",
        "                optimizer=optimizer,\n",
        "                scheduler=None,\n",
        "                train_sequence=train_int_sequence_I,\n",
        "                val_sequence=val_int_sequence_I,\n",
        "                sequence_length=50,\n",
        "                batch_size=128,\n",
        "                epochs=30,\n",
        "                patience=10,\n",
        "                name='lstm_with_learnable_embeddings')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss, val_loss, checkpoints = instance.training()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jF3S00LHFngG",
        "outputId": "2e5a3f82-a803-4db0-9c64-223eb7401cbf"
      },
      "id": "jF3S00LHFngG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training..\n",
            "Epoch: 1/50 - Perplexity: training 310.144, validation 133.036\n",
            "Epoch: 2/50 - Perplexity: training 178.727, validation 105.156 - E.S. checkpoint\n",
            "Epoch: 3/50 - Perplexity: training 135.101, validation 93.555 - E.S. checkpoint\n",
            "Epoch: 4/50 - Perplexity: training 106.588, validation 86.646 - E.S. checkpoint\n",
            "Epoch: 5/50 - Perplexity: training 85.351, validation 82.556 - E.S. checkpoint\n",
            "Epoch: 6/50 - Perplexity: training 68.566, validation 81.623 - E.S. checkpoint\n",
            "Epoch: 7/50 - Perplexity: training 55.260, validation 83.485\n",
            "Epoch: 8/50 - Perplexity: training 45.303, validation 87.138\n",
            "Epoch: 9/50 - Perplexity: training 37.053, validation 93.132\n",
            "Epoch: 10/50 - Perplexity: training 30.764, validation 100.262\n",
            "Epoch: 11/50 - Perplexity: training 25.695, validation 111.074\n",
            "Epoch: 12/50 - Perplexity: training 21.778, validation 123.306\n",
            "Epoch: 13/50 - Perplexity: training 18.626, validation 140.248\n",
            "Epoch: 14/50 - Perplexity: training 15.982, validation 155.976\n",
            "Epoch: 15/50 - Perplexity: training 13.949, validation 175.644\n",
            "Epoch: 16/50 - Perplexity: training 12.404, validation 205.427\n",
            "Training complete !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "instance = Train(model=model,\n",
        "                model_type = 'lstm',\n",
        "                loss_fct=criterion,\n",
        "                optimizer=optimizer,\n",
        "                scheduler=None,\n",
        "                train_sequence=val_int_sequence_I,\n",
        "                val_sequence=None,\n",
        "                sequence_length=50,\n",
        "                batch_size=128,\n",
        "                epochs=6,\n",
        "                patience=None,\n",
        "                name=None)\n",
        "\n",
        "train_loss_of_val_data = instance.training()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdLfHZP7Fnit",
        "outputId": "384a6577-789d-4b70-b483-899575f9d81a"
      },
      "id": "QdLfHZP7Fnit",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda:0\n",
            "Starting training..\n",
            "No validation data is used.\n",
            "Epoch: 1/6 - Perplexity: training 159.130\n",
            "Epoch: 2/6 - Perplexity: training 52.802\n",
            "Epoch: 3/6 - Perplexity: training 30.531\n",
            "Epoch: 4/6 - Perplexity: training 20.326\n",
            "Epoch: 5/6 - Perplexity: training 14.521\n",
            "Epoch: 6/6 - Perplexity: training 10.756\n",
            "Training complete !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, 'model_epoch6_lstm_with_learnable_embeddings.pth')"
      ],
      "metadata": {
        "id": "6NNVTsKSth10"
      },
      "id": "6NNVTsKSth10",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perplexity_network_model(test_sequence_of_integers = test_int_sequence_I,\n",
        "                        sequence_length = 50,\n",
        "                        model = model,\n",
        "                        model_type = 'lstm',\n",
        "                        loss_fct = nn.CrossEntropyLoss(),\n",
        "                        vocab_size = len(vocabulary_I))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwnqNKPNFnlo",
        "outputId": "b5174874-e9b7-4915-b3be-2520d6dc0f8a"
      },
      "id": "OwnqNKPNFnlo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "248.95954826843968"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8w-Xug5efKwP",
      "metadata": {
        "id": "8w-Xug5efKwP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "FMioYyfHQLFp",
      "metadata": {
        "id": "FMioYyfHQLFp"
      },
      "source": [
        "#### case II - model with pre-trained GloVe embeddings: further preprocessing, model training and perplexity (all variable names end in 'II')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6iDwGHocxEbX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iDwGHocxEbX",
        "outputId": "c05c9c40-37b3-48a4-eac7-8a71b116b868"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EVUA2QHuxEeV",
      "metadata": {
        "id": "EVUA2QHuxEeV"
      },
      "outputs": [],
      "source": [
        "# Load GloVe 300-dim embeddings into word_embeddings dictionary of (word,vector) pairs\n",
        "import os\n",
        "\n",
        "glove_dir ='/content/drive/MyDrive/Colab_Notebooks/GitHub_language_models_repo'\n",
        "\n",
        "word_embeddings = {} # dictionary with (word, embedding) items\n",
        "\n",
        "with open(os.path.join(glove_dir, 'glove.6B.300d.txt')) as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        embedding = torch.tensor([float(val) for val in values[1:]])\n",
        "        word_embeddings[word] = embedding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Do GloVe embeddings have representations for all tokens in the vocabulary_I?\n",
        "intersection = set(word_embeddings.keys()) & vocabulary_I\n",
        "words_not_in_glove = vocabulary_I - intersection\n",
        "len(words_not_in_glove) #'<unk>', '<eos>' included"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymlO5QYnULuA",
        "outputId": "d441ccd1-692a-421a-cf26-41c90a938cb2"
      },
      "id": "ymlO5QYnULuA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How to deal with words_not_in_glove ?\n",
        "# One approach, is to assign them to the '<unk>' token. This results in a reduced vocabulary.\n",
        "\n",
        "vocabulary_II = vocabulary_I - words_not_in_glove\n",
        "vocabulary_II.add('<unk>')\n",
        "vocabulary_II.add('<eos>')\n",
        "\n",
        "assert len(vocabulary_II) == len(vocabulary_I) - len(words_not_in_glove) + 2 # 3225 = 3259 - 36 + 2"
      ],
      "metadata": {
        "id": "gXO_8eZVnCl8"
      },
      "id": "gXO_8eZVnCl8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we replace with <unk> tokens that are not included in the vocabulary_II as well\n",
        "train_sentences_II = unk_for_reduced_vocab(train_sentences_I, vocabulary_II)"
      ],
      "metadata": {
        "id": "mKmkfvvKqazx"
      },
      "id": "mKmkfvvKqazx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# not the same index assignments every time i run the cell\n",
        "word_to_index_II = {word: idx for idx, word in enumerate(vocabulary_II)}\n",
        "index_to_word_II = {idx: word for word, idx in word_to_index_II.items()}\n",
        "\n",
        "with open('lstm_caseII_word_index_mappings.pickle', 'wb') as f:\n",
        "        pickle.dump([word_to_index_II, index_to_word_II], f)"
      ],
      "metadata": {
        "id": "AcS9o6sLqa4H"
      },
      "id": "AcS9o6sLqa4H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training sequence of indices\n",
        "train_int_sequence_II = tokens_to_indices(word_to_index_II, train_sentences_II)\n",
        "\n",
        "# validation sequence of indices\n",
        "val_sentences_II = replace_with_unk_for_testing(vocabulary_II, lower_val_treebank)\n",
        "val_int_sequence_II = tokens_to_indices(word_to_index_II, val_sentences_II)\n",
        "\n",
        "# testing sequence of indices\n",
        "test_sentences_II = replace_with_unk_for_testing(vocabulary_II, lower_test_treebank)\n",
        "test_int_sequence_II = tokens_to_indices(word_to_index_II, test_sentences_II)\n",
        "\n",
        "len(train_int_sequence_II), len(val_int_sequence_II), len(test_int_sequence_II)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x47fWPrVqa6w",
        "outputId": "2ef19ced-c275-4ada-ad60-923a7b0ceeb2"
      },
      "id": "x47fWPrVqa6w",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(82372, 8003, 8319)"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(set(train_int_sequence_II)), len(set(val_int_sequence_II)), len(set(test_int_sequence_II))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeUyBWovuw2z",
        "outputId": "4d6c76a7-4ad0-4ef2-8c35-a7233c64a2f7"
      },
      "id": "QeUyBWovuw2z",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3225, 1160, 1269)"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the maximum and minimum values in GloVe vectors\n",
        "\n",
        "tensors = list(word_embeddings.values())\n",
        "\n",
        "tensors_tensor = torch.stack(tensors)\n",
        "\n",
        "max_value = torch.max(tensors_tensor)\n",
        "min_value = torch.min(tensors_tensor)\n",
        "\n",
        "max_value.item(), min_value.item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvkzwH8zD5dC",
        "outputId": "11701022-d973-4ca3-c17d-231893bceb92"
      },
      "id": "DvkzwH8zD5dC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3.25819993019104, -3.0638999938964844)"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create embedding layer weights\n",
        "\n",
        "vocabII_size = len(vocabulary_II)\n",
        "embeddings = torch.zeros(vocabII_size, embedding_dim)\n",
        "\n",
        "# put the glove embeddings in the embeddings matrix\n",
        "for (word, index) in word_to_index_II.items():\n",
        "    if word not in ['<unk>', '<eos>']:\n",
        "        embeddings[index] = word_embeddings[word]\n",
        "\n",
        "eos_index = word_to_index_II['<eos>']\n",
        "all_vectors = list(word_embeddings.values())\n",
        "embeddings[eos_index] = torch.mean(torch.stack(all_vectors), dim=0)\n",
        "\n",
        "unk_index = word_to_index_II['<unk>']\n",
        "embeddings[unk_index] = (max_value.item() - min_value.item()) * torch.rand(embedding_dim) + min_value.item()"
      ],
      "metadata": {
        "id": "qI2SEqyaqa9K"
      },
      "id": "qI2SEqyaqa9K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ae-cz8b9hSPs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ae-cz8b9hSPs",
        "outputId": "093295a7-3116-44a6-a7ed-bcd991ab3c54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of trainable parameters: 1926553\n"
          ]
        }
      ],
      "source": [
        "# Create the model, loss function, and optimizer\n",
        "model = LSTMModel(vocabII_size, embedding_dim, hidden_dim, num_layers, dropout_rate, embeddings)\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'No. of trainable parameters: {num_params}')\n",
        "\n",
        "#model training hyperparams\n",
        "learning_rate = 0.001\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "instance = Train(model=model,\n",
        "                model_type = 'lstm',\n",
        "                loss_fct=criterion,\n",
        "                optimizer=optimizer,\n",
        "                scheduler=None,\n",
        "                train_sequence=train_int_sequence_II,\n",
        "                val_sequence=val_int_sequence_II,\n",
        "                sequence_length=50,\n",
        "                batch_size=128,\n",
        "                epochs=30,\n",
        "                patience=10,\n",
        "                name='lstm_with_glove_embeddings')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFCrWE-q5dbD",
        "outputId": "0f8d2705-5a58-48d6-e503-9d1ab6c10c18"
      },
      "id": "hFCrWE-q5dbD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss, val_loss, checkpoints = instance.training()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbOEongHH1al",
        "outputId": "b4669f6d-9180-4ec7-a024-e567f0a15030"
      },
      "id": "jbOEongHH1al",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training..\n",
            "Epoch: 1/50 - Perplexity: training 347.218, validation 153.147\n",
            "Epoch: 2/50 - Perplexity: training 205.620, validation 119.665 - E.S. checkpoint\n",
            "Epoch: 3/50 - Perplexity: training 158.176, validation 99.479 - E.S. checkpoint\n",
            "Epoch: 4/50 - Perplexity: training 128.392, validation 88.739 - E.S. checkpoint\n",
            "Epoch: 5/50 - Perplexity: training 107.411, validation 82.375 - E.S. checkpoint\n",
            "Epoch: 6/50 - Perplexity: training 90.900, validation 77.508 - E.S. checkpoint\n",
            "Epoch: 7/50 - Perplexity: training 78.735, validation 74.539 - E.S. checkpoint\n",
            "Epoch: 8/50 - Perplexity: training 67.797, validation 73.111 - E.S. checkpoint\n",
            "Epoch: 9/50 - Perplexity: training 59.600, validation 73.099 - E.S. checkpoint\n",
            "Epoch: 10/50 - Perplexity: training 52.220, validation 72.949 - E.S. checkpoint\n",
            "Epoch: 11/50 - Perplexity: training 46.165, validation 73.987\n",
            "Epoch: 12/50 - Perplexity: training 40.645, validation 75.250\n",
            "Epoch: 13/50 - Perplexity: training 36.464, validation 76.762\n",
            "Epoch: 14/50 - Perplexity: training 32.512, validation 79.594\n",
            "Epoch: 15/50 - Perplexity: training 29.135, validation 83.011\n",
            "Epoch: 16/50 - Perplexity: training 26.327, validation 85.903\n",
            "Epoch: 17/50 - Perplexity: training 23.850, validation 89.843\n",
            "Epoch: 18/50 - Perplexity: training 21.934, validation 91.796\n",
            "Epoch: 19/50 - Perplexity: training 20.112, validation 96.943\n",
            "Epoch: 20/50 - Perplexity: training 18.371, validation 102.302\n",
            "Training complete !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "instance = Train(model=model,\n",
        "                model_type = 'lstm',\n",
        "                loss_fct=criterion,\n",
        "                optimizer=optimizer,\n",
        "                scheduler=None,\n",
        "                train_sequence=val_int_sequence_II,\n",
        "                val_sequence=None,\n",
        "                sequence_length=50,\n",
        "                batch_size=128,\n",
        "                epochs=10,\n",
        "                patience=None,\n",
        "                name=None)\n",
        "\n",
        "train_loss_of_val_data = instance.training()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQMYnr_TH1ek",
        "outputId": "3d904322-2d86-4308-ee51-2e57b6083c7b"
      },
      "id": "XQMYnr_TH1ek",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda:0\n",
            "Starting training..\n",
            "No validation data is used.\n",
            "Epoch: 1/10 - Perplexity: training 104.723\n",
            "Epoch: 2/10 - Perplexity: training 55.702\n",
            "Epoch: 3/10 - Perplexity: training 35.913\n",
            "Epoch: 4/10 - Perplexity: training 25.857\n",
            "Epoch: 5/10 - Perplexity: training 20.086\n",
            "Epoch: 6/10 - Perplexity: training 15.860\n",
            "Epoch: 7/10 - Perplexity: training 12.778\n",
            "Epoch: 8/10 - Perplexity: training 10.504\n",
            "Epoch: 9/10 - Perplexity: training 8.705\n",
            "Epoch: 10/10 - Perplexity: training 7.483\n",
            "Training complete !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, 'model_epoch10_lstm_with_glove_embeddings.pth')"
      ],
      "metadata": {
        "id": "Itn-JK2AOM__"
      },
      "id": "Itn-JK2AOM__",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perplexity_network_model(test_sequence_of_integers = test_int_sequence_II,\n",
        "                        sequence_length = 50,\n",
        "                        model = model,\n",
        "                        model_type = 'lstm',\n",
        "                        loss_fct = nn.CrossEntropyLoss(),\n",
        "                        vocab_size = len(vocabulary_II))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_0x3x49H1ia",
        "outputId": "0a3175ab-1adb-4b90-a046-21fb159ceae6"
      },
      "id": "Z_0x3x49H1ia",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "195.7221341660012"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t8u98WF2H1lz"
      },
      "id": "t8u98WF2H1lz",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "WZFdTVnWvYHG",
        "VG3HM5u6vcBv",
        "Z12KumNdn827",
        "FMioYyfHQLFp"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}